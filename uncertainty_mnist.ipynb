{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Find $p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathbf{X}, \\mathbf{Y})=\\int{p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathbf{w}) p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y})d\\mathbf{w}}$\n",
    "### Variational Inference\n",
    "### $$q(\\mathbf{y}^*|\\mathbf{x}^*)=\\int{p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathbf{w}) q(\\mathbf{w})d\\mathbf{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainties\n",
    "## Let $y=f(\\mathbf{x})+\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_a^2)$\n",
    "### $$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y-\\hat{f}(\\mathbf{x})]^2\n",
    "&=\\mathbb{E}[y-f(\\mathbf{x})+f(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=\\mathbb{E}[\\epsilon+f(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=\\mathbb{E}[\\epsilon]^2+\\mathbb{E}[f(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=\\sigma_a^2 + \\mathbb{E}[f(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=\\sigma_a^2 + \\sigma_e^2\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### And, $$\n",
    "\\begin{align*}\n",
    "\\sigma_e^2 &= \\mathbb{E}[f(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=\\mathbb{E}[f(\\mathbf{x})-\\bar{f}(\\mathbf{x})+\\bar{f}(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=\\mathbb{E}[f(\\mathbf{x})-\\bar{f}(\\mathbf{x})]^2+\\mathbb{E}[\\bar{f}(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&=bias(\\hat{f}(\\mathbf{x}))^2+Var(\\hat{f}(\\mathbf{x}))\\\\\\\\\n",
    "&\\text{Assuming $\\hat{f}(\\mathbf{x})$ is a unbiased estimator, we have}\\\\\n",
    "&=Var(\\hat{f}(\\mathbf{x}))\\\\\n",
    "&=\\mathbb{E}[\\hat{f}(\\mathbf{x})^2]-\\mathbb{E}[\\hat{f}(\\mathbf{x})]^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence, $$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y-\\hat{f}(\\mathbf{x})]^2\n",
    "&=\\sigma_a^2 + \\sigma_e^2\\\\\n",
    "&=\\sigma_a^2 + \\mathbb{E}[\\hat{f}(\\mathbf{x})^2]-\\mathbb{E}[\\hat{f}(\\mathbf{x})]^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will estimate the uncertainties using MC Dropout as follows $$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y-\\hat{f}(\\mathbf{x})]^2\n",
    "&=\\sigma_a^2 + \\sigma_e^2\\\\\n",
    "&=\\sigma_a^2 + \\mathbb{E}[\\hat{f}(\\mathbf{x})^2]-\\mathbb{E}[\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&\\approx\\hat{\\sigma_a}^2 + \\frac{1}{T}\\sum_{t=1}^{T}{\\hat{f_t}(\\mathbf{x})^2}-\\bigg(\\frac{1}{T}\\sum_{t=1}^{T}{\\hat{f_t}(\\mathbf{x})\\bigg)}^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Sampling $$\\mathbf{w}_t\\sim q(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "\n",
    "train_img = mnist.train._images\n",
    "train_label = mnist.train._labels.copy()\n",
    "test_img = mnist.test._images\n",
    "test_label = mnist.test._labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen label or Noise label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 제거\n",
    "# not_8 = np.where(train_label != 8)[0]\n",
    "#\n",
    "# train_img = train_img[not_8, :]\n",
    "# train_label = train_label[not_8]\n",
    "\n",
    "# 8에 노이즈\n",
    "# idx_8 = np.where(train_label == 8)[0]\n",
    "# train_label[idx_8] = np.random.choice(np.arange(10), idx_8.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_moment = 0.9\n",
    "n_mc_dropout = 50 # Dropout의 횟수\n",
    "\n",
    "batch_size_tr = 32\n",
    "batch_size_ts = 64\n",
    "model_name = 'MNIST_Uncertainty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initializer & Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "regularizer = tf.contrib.layers.l2_regularizer(10**-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "y = tf.placeholder(tf.int32, [None], name='target')\n",
    "\n",
    "training = tf.placeholder(tf.bool, name='training')\n",
    "conv_keep_prob = tf.placeholder(tf.float32, name='conv_keep_prob')\n",
    "fc_keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy input data for MC integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repeat = tf.transpose(tf.tile(tf.expand_dims(x, 0), [n_mc_dropout, 1, 1]), [1, 0, 2])\n",
    "x_img = tf.reshape(repeat, [-1, 28, 28, 1])\n",
    "\n",
    "y_repeat = tf.transpose(tf.tile(tf.expand_dims(y, 0), [n_mc_dropout, 1]))\n",
    "y_repeat = tf.reshape(y_repeat, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1'):\n",
    "    net = tf.layers.conv2d(x_img, 32, kernel_size=[3, 3], strides=[2, 2], padding='SAME',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = tf.layers.batch_normalization(net, momentum=bn_moment, training=training)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.layers.dropout(net, conv_keep_prob, training=training)\n",
    "\n",
    "with tf.variable_scope('conv2'):\n",
    "    net = tf.layers.conv2d(net, 64, kernel_size=[3, 3], padding='SAME',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = tf.layers.batch_normalization(net, momentum=bn_moment, training=training)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.layers.dropout(net, conv_keep_prob, training=training)\n",
    "\n",
    "with tf.variable_scope('conv3'):\n",
    "    net = tf.layers.conv2d(net, 64, kernel_size=[3, 3], padding='SAME',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = tf.layers.batch_normalization(net, momentum=bn_moment, training=training)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.layers.dropout(net, conv_keep_prob, training=training)\n",
    "\n",
    "with tf.variable_scope('conv4'):\n",
    "    net = tf.layers.conv2d(net, 128, kernel_size=[3, 3], strides=[2, 2], padding='SAME',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = tf.layers.batch_normalization(net, momentum=bn_moment, training=training)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.layers.dropout(net, conv_keep_prob, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc1'):\n",
    "    net = tf.layers.conv2d(net, 512, kernel_size=[7, 7], padding='VALID',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = tf.layers.batch_normalization(net, momentum=bn_moment, training=training)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.layers.dropout(net, fc_keep_prob, training=training)\n",
    "\n",
    "with tf.variable_scope('fc2'):\n",
    "    net = tf.layers.conv2d(net, 512, kernel_size=[1, 1], padding='VALID',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = tf.layers.batch_normalization(net, momentum=bn_moment, training=training)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.layers.dropout(net, fc_keep_prob, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('logit'):\n",
    "    net = tf.layers.conv2d(net, 11, kernel_size=[1, 1], padding='VALID',\n",
    "                           kernel_initializer=weight_initializer,\n",
    "                           kernel_regularizer=regularizer, bias_regularizer=regularizer)\n",
    "    net = net[:, 0, 0, :]\n",
    "    \n",
    "    mu = net[..., :10]\n",
    "    s = net[..., -1]\n",
    "    sigma = tf.exp(s/2)\n",
    "    target_shape = tf.shape(mu)\n",
    "    \n",
    "    mc_out = mu + tf.random_normal(target_shape)*tf.expand_dims(sigma, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the paper](https://arxiv.org/abs/1703.04977), loss has to be calculated as follow.\n",
    "\\begin{align*}\n",
    "p(y=c|\\mathbf{x}, \\mathbf{X}, \\mathbf{Y})\n",
    "&=\\int{Softmax(f_\\mathbf{w}(\\mathbf{x}))p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y})d\\mathbf{w}}\\\\\n",
    "&\\approx \\int{Softmax(f_\\mathbf{w}(\\mathbf{x}))q(\\mathbf{w})d\\mathbf{w}}\\\\\n",
    "&\\approx \\frac{1}{T}\\sum_{t=1}^{T}{Softmax(f_{\\mathbf{w}_t}(\\mathbf{x}))}\n",
    "\\end{align*}\n",
    "Sampling $$\\mathbf{w}_t \\sim q(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, $$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}=-\\sum_i{\\log{\\frac{1}{T}\\sum_t{Softmax\\big(f_{\\mathbf{w}_t}(\\mathbf{x})\\big)}}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, following code minimize\n",
    "$$\n",
    "\\mathcal{L^{alt}}=-\\sum_i{\\frac{1}{T}\\sum_t{\\log{Softmax\\big(f_{\\mathbf{w}_t}(\\mathbf{x})\\big)}}}\n",
    "$$\n",
    "with weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    # prob = tf.reduce_mean((tf.nn.softmax(mc_out), 0)\n",
    "\n",
    "    # idx = tf.concat([tf.expand_dims(tf.range(target_shape[0]*target_shape[1]), -1), tf.expand_dims(y_repeat, -1)], -1)\n",
    "    # loss = tf.gather_nd(tf.reduce_logsumexp(mc_out - tf.expand_dims(tf.reduce_logsumexp(mc_out, -1), -1), 1), idx)\n",
    "    # loss -= tf.log(float(n_mc_dropout)) # 학습상 상수이기에 필요 없다.\n",
    "\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_repeat, logits=mc_out)\n",
    "\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    loss += tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    tf.summary.scalar('total_loss', loss)\n",
    "\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        rmsprop = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        sgd = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Uncertainties\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[y-\\hat{f}(\\mathbf{x})]^2\n",
    "&=\\sigma_a^2 + \\sigma_e^2\\\\\n",
    "&=\\sigma_a^2 + \\mathbb{E}[\\hat{f}(\\mathbf{x})^2]-\\mathbb{E}[\\hat{f}(\\mathbf{x})]^2\\\\\n",
    "&\\approx\\hat{\\sigma_a}^2 + \\frac{1}{T}\\sum_{t=1}^{T}{\\hat{f_t}(\\mathbf{x})^2}-\\bigg(\\frac{1}{T}\\sum_{t=1}^{T}{\\hat{f_t}(\\mathbf{x})\\bigg)}^2\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('uncertainties'):\n",
    "    mu = tf.reshape(mu, [-1, 50, 10])\n",
    "    sigma = tf.reshape(sigma, [-1, 50])\n",
    "    epistemic = tf.reduce_mean(mu**2, 1) - tf.reduce_mean(mu, 1)**2\n",
    "    aleatoric = tf.reduce_mean(sigma**2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(y=c|\\mathbf{x}, \\mathbf{X}, \\mathbf{Y})\n",
    "&=\\int{Softmax(f_\\mathbf{w}(\\mathbf{x}))p(\\mathbf{w}|\\mathbf{X}, \\mathbf{Y})d\\mathbf{w}}\\\\\n",
    "&\\approx \\int{Softmax(f_\\mathbf{w}(\\mathbf{x}))q(\\mathbf{w})d\\mathbf{w}}\\\\\n",
    "&\\approx \\frac{1}{T}\\sum_{t=1}^{T}{Softmax(f_{\\mathbf{w}_t}(\\mathbf{x}))}\n",
    "\\end{align*}\n",
    "Sampling $$\\mathbf{w}_t \\sim q(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('y_prob'):\n",
    "    # expected_prob = tf.reduce_mean(tf.exp(mc_out - tf.expand_dims(tf.reduce_logsumexp(mc_out, -1), -1)), 1)\n",
    "\n",
    "    mc_out_reshape = tf.reshape(mc_out, [-1, 50, 10])\n",
    "    expected_prob = tf.reduce_mean(tf.exp(mc_out_reshape - tf.expand_dims(tf.reduce_logsumexp(mc_out_reshape, -1), -1)), 1)\n",
    "\n",
    "    y_pred = tf.argmax(expected_prob, 1, output_type=tf.int32)\n",
    "    tf.summary.histogram('probability', expected_prob)\n",
    "    correct_prediction = tf.equal(y_pred, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "def feed_dict(train, batch_size, lr=0.01):\n",
    "    if train:\n",
    "        batch_idx = np.random.choice(train_img.shape[0], batch_size, False)\n",
    "        xs = train_img[batch_idx, :]\n",
    "        ys = train_label[batch_idx]\n",
    "        tr = True\n",
    "    else:\n",
    "        batch_idx = np.random.choice(test_img.shape[0], batch_size, False)\n",
    "        xs = test_img[batch_idx, :]\n",
    "        ys = test_label[batch_idx]\n",
    "        tr = True\n",
    "    return {x: xs, y: ys, training: tr, learning_rate: lr, conv_keep_prob: 0.8, fc_keep_prob: 0.8}\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "train_writer = tf.summary.FileWriter(model_name + '/train',sess.graph)\n",
    "test_writer = tf.summary.FileWriter(model_name + '/test')\n",
    "\n",
    "optimizer = rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and loss at step 10: 0.03125 4.27133\n",
      "Accuracy and loss at step 20: 0.125 3.97181\n",
      "Accuracy and loss at step 30: 0.09375 3.73276\n",
      "Accuracy and loss at step 40: 0.109375 3.54113\n",
      "Accuracy and loss at step 50: 0.0625 3.50008\n",
      "Accuracy and loss at step 60: 0.125 3.01057\n",
      "Accuracy and loss at step 70: 0.09375 3.24462\n",
      "Accuracy and loss at step 80: 0.09375 3.20744\n",
      "Accuracy and loss at step 90: 0.21875 2.81554\n",
      "Accuracy and loss at step 100: 0.203125 2.80749\n",
      "Accuracy and loss at step 110: 0.140625 2.61759\n",
      "Accuracy and loss at step 120: 0.4375 2.03378\n",
      "Accuracy and loss at step 130: 0.625 1.53541\n",
      "Accuracy and loss at step 140: 0.71875 1.49552\n",
      "Accuracy and loss at step 150: 0.5625 1.5717\n",
      "Accuracy and loss at step 160: 0.65625 1.47854\n",
      "Accuracy and loss at step 170: 0.765625 1.26963\n",
      "Accuracy and loss at step 180: 0.71875 1.58199\n",
      "Accuracy and loss at step 190: 0.890625 1.02184\n",
      "Accuracy and loss at step 200: 0.828125 1.15517\n",
      "Accuracy and loss at step 210: 0.765625 1.57003\n",
      "Accuracy and loss at step 220: 0.921875 1.11301\n",
      "Accuracy and loss at step 230: 0.8125 1.33778\n",
      "Accuracy and loss at step 240: 0.921875 1.1443\n",
      "Accuracy and loss at step 250: 0.890625 1.39148\n",
      "Accuracy and loss at step 260: 0.875 1.23334\n",
      "Accuracy and loss at step 270: 0.71875 1.57887\n",
      "Accuracy and loss at step 280: 0.84375 1.41128\n",
      "Accuracy and loss at step 290: 0.890625 1.17745\n",
      "Accuracy and loss at step 300: 0.9375 1.15632\n",
      "Accuracy and loss at step 310: 0.921875 1.15485\n",
      "Accuracy and loss at step 320: 0.859375 1.20086\n",
      "Accuracy and loss at step 330: 0.9375 1.20139\n",
      "Accuracy and loss at step 340: 0.90625 1.24877\n",
      "Accuracy and loss at step 350: 0.90625 1.13263\n",
      "Accuracy and loss at step 360: 0.875 1.16803\n",
      "Accuracy and loss at step 370: 0.953125 0.993593\n",
      "Accuracy and loss at step 380: 0.90625 1.33268\n",
      "Accuracy and loss at step 390: 0.890625 1.23755\n",
      "Accuracy and loss at step 400: 0.921875 1.14289\n",
      "Accuracy and loss at step 410: 0.90625 1.21692\n",
      "Accuracy and loss at step 420: 0.9375 1.23452\n",
      "Accuracy and loss at step 430: 0.9375 1.22081\n",
      "Accuracy and loss at step 440: 0.9375 1.23682\n",
      "Accuracy and loss at step 450: 0.796875 1.4686\n",
      "Accuracy and loss at step 460: 0.953125 1.01959\n",
      "Accuracy and loss at step 470: 0.984375 0.945643\n",
      "Accuracy and loss at step 480: 0.984375 1.00325\n",
      "Accuracy and loss at step 490: 0.875 1.33409\n",
      "Accuracy and loss at step 500: 0.9375 1.33512\n",
      "Accuracy and loss at step 510: 0.890625 1.24661\n",
      "Accuracy and loss at step 520: 0.921875 1.08372\n",
      "Accuracy and loss at step 530: 0.9375 1.16516\n",
      "Accuracy and loss at step 540: 0.9375 1.16234\n",
      "Accuracy and loss at step 550: 0.953125 1.00904\n",
      "Accuracy and loss at step 560: 0.9375 1.1777\n",
      "Accuracy and loss at step 570: 0.9375 1.10094\n",
      "Accuracy and loss at step 580: 0.9375 1.07099\n",
      "Accuracy and loss at step 590: 0.953125 1.11606\n",
      "Accuracy and loss at step 600: 0.921875 1.17037\n",
      "Accuracy and loss at step 610: 0.921875 1.17666\n",
      "Accuracy and loss at step 620: 0.921875 1.071\n",
      "Accuracy and loss at step 630: 0.96875 1.00624\n",
      "Accuracy and loss at step 640: 0.921875 1.22964\n",
      "Accuracy and loss at step 650: 0.921875 1.20741\n",
      "Accuracy and loss at step 660: 0.9375 1.21298\n",
      "Accuracy and loss at step 670: 0.953125 1.02849\n",
      "Accuracy and loss at step 680: 0.9375 1.03411\n",
      "Accuracy and loss at step 690: 0.96875 1.11906\n",
      "Accuracy and loss at step 700: 0.9375 1.04001\n",
      "Accuracy and loss at step 710: 0.953125 1.05233\n",
      "Accuracy and loss at step 720: 0.9375 1.15768\n",
      "Accuracy and loss at step 730: 0.90625 1.11424\n",
      "Accuracy and loss at step 740: 0.890625 1.2726\n",
      "Accuracy and loss at step 750: 0.90625 1.277\n",
      "Accuracy and loss at step 760: 0.96875 1.04203\n",
      "Accuracy and loss at step 770: 0.890625 1.20538\n",
      "Accuracy and loss at step 780: 0.90625 1.17983\n",
      "Accuracy and loss at step 790: 0.9375 1.24219\n",
      "Accuracy and loss at step 800: 0.96875 1.01592\n",
      "Accuracy and loss at step 810: 0.875 1.37145\n",
      "Accuracy and loss at step 820: 0.9375 1.09974\n",
      "Accuracy and loss at step 830: 0.96875 0.963865\n",
      "Accuracy and loss at step 840: 0.953125 1.03448\n",
      "Accuracy and loss at step 850: 0.9375 1.12688\n",
      "Accuracy and loss at step 860: 0.9375 1.04647\n",
      "Accuracy and loss at step 870: 0.9375 1.07134\n",
      "Accuracy and loss at step 880: 0.96875 0.951343\n",
      "Accuracy and loss at step 890: 0.96875 1.04802\n",
      "Accuracy and loss at step 900: 0.9375 1.0626\n",
      "Accuracy and loss at step 910: 0.875 1.16296\n",
      "Accuracy and loss at step 920: 0.953125 1.16359\n",
      "Accuracy and loss at step 930: 0.96875 1.03464\n",
      "Accuracy and loss at step 940: 0.96875 1.00436\n",
      "Accuracy and loss at step 950: 1.0 0.837542\n",
      "Accuracy and loss at step 960: 0.984375 1.00638\n",
      "Accuracy and loss at step 970: 0.984375 0.943446\n",
      "Accuracy and loss at step 980: 0.953125 1.17241\n",
      "Accuracy and loss at step 990: 0.953125 0.995228\n",
      "Accuracy and loss at step 1000: 0.953125 1.07958\n",
      "Accuracy and loss at step 1010: 0.875 1.39259\n",
      "Accuracy and loss at step 1020: 1.0 0.906307\n",
      "Accuracy and loss at step 1030: 0.984375 0.973082\n",
      "Accuracy and loss at step 1040: 0.984375 1.01977\n",
      "Accuracy and loss at step 1050: 0.90625 1.07157\n",
      "Accuracy and loss at step 1060: 0.953125 0.950021\n",
      "Accuracy and loss at step 1070: 0.90625 1.19107\n",
      "Accuracy and loss at step 1080: 0.875 1.30685\n",
      "Accuracy and loss at step 1090: 0.90625 1.29869\n",
      "Accuracy and loss at step 1100: 0.96875 0.998431\n",
      "Accuracy and loss at step 1110: 0.984375 0.876153\n",
      "Accuracy and loss at step 1120: 0.984375 1.043\n",
      "Accuracy and loss at step 1130: 0.96875 1.01653\n",
      "Accuracy and loss at step 1140: 0.96875 0.968024\n",
      "Accuracy and loss at step 1150: 0.96875 0.97452\n",
      "Accuracy and loss at step 1160: 0.921875 1.14723\n",
      "Accuracy and loss at step 1170: 0.96875 0.895829\n",
      "Accuracy and loss at step 1180: 0.9375 1.06089\n",
      "Accuracy and loss at step 1190: 0.9375 1.04744\n",
      "Accuracy and loss at step 1200: 0.890625 1.28015\n",
      "Accuracy and loss at step 1210: 0.921875 1.32153\n",
      "Accuracy and loss at step 1220: 0.921875 1.13861\n",
      "Accuracy and loss at step 1230: 0.953125 1.03826\n",
      "Accuracy and loss at step 1240: 0.9375 0.957283\n",
      "Accuracy and loss at step 1250: 0.90625 1.11447\n",
      "Accuracy and loss at step 1260: 0.9375 0.980075\n",
      "Accuracy and loss at step 1270: 0.953125 0.962948\n",
      "Accuracy and loss at step 1280: 0.921875 1.22588\n",
      "Accuracy and loss at step 1290: 0.953125 0.988867\n",
      "Accuracy and loss at step 1300: 1.0 0.794032\n",
      "Accuracy and loss at step 1310: 0.96875 1.00607\n",
      "Accuracy and loss at step 1320: 0.96875 0.998388\n",
      "Accuracy and loss at step 1330: 0.953125 1.01811\n",
      "Accuracy and loss at step 1340: 0.921875 1.00316\n",
      "Accuracy and loss at step 1350: 0.90625 1.10234\n",
      "Accuracy and loss at step 1360: 0.9375 1.14558\n",
      "Accuracy and loss at step 1370: 0.9375 1.03763\n",
      "Accuracy and loss at step 1380: 0.984375 0.910155\n",
      "Accuracy and loss at step 1390: 0.984375 0.945423\n",
      "Accuracy and loss at step 1400: 0.984375 0.949899\n",
      "Accuracy and loss at step 1410: 0.96875 1.13149\n",
      "Accuracy and loss at step 1420: 0.875 1.09492\n",
      "Accuracy and loss at step 1430: 0.859375 1.31855\n",
      "Accuracy and loss at step 1440: 0.9375 0.997132\n",
      "Accuracy and loss at step 1450: 0.984375 0.873316\n",
      "Accuracy and loss at step 1460: 0.953125 0.94718\n",
      "Accuracy and loss at step 1470: 0.921875 1.03069\n",
      "Accuracy and loss at step 1480: 0.9375 0.969396\n",
      "Accuracy and loss at step 1490: 0.9375 1.03552\n",
      "Accuracy and loss at step 1500: 0.9375 1.12913\n",
      "Accuracy and loss at step 1510: 0.90625 1.18559\n",
      "Accuracy and loss at step 1520: 0.9375 1.09444\n",
      "Accuracy and loss at step 1530: 0.953125 1.00177\n",
      "Accuracy and loss at step 1540: 1.0 0.988469\n",
      "Accuracy and loss at step 1550: 0.921875 1.141\n",
      "Accuracy and loss at step 1560: 0.84375 1.21755\n",
      "Accuracy and loss at step 1570: 0.96875 0.890035\n",
      "Accuracy and loss at step 1580: 0.9375 1.13048\n",
      "Accuracy and loss at step 1590: 0.921875 1.08346\n",
      "Accuracy and loss at step 1600: 0.921875 1.10836\n",
      "Accuracy and loss at step 1610: 0.96875 0.922308\n",
      "Accuracy and loss at step 1620: 0.890625 1.10576\n",
      "Accuracy and loss at step 1630: 0.921875 1.06494\n",
      "Accuracy and loss at step 1640: 0.953125 0.966307\n",
      "Accuracy and loss at step 1650: 1.0 0.805874\n",
      "Accuracy and loss at step 1660: 0.921875 1.12548\n",
      "Accuracy and loss at step 1670: 0.953125 1.02416\n",
      "Accuracy and loss at step 1680: 0.984375 0.923806\n",
      "Accuracy and loss at step 1690: 0.921875 1.08273\n",
      "Accuracy and loss at step 1700: 0.96875 0.825072\n",
      "Accuracy and loss at step 1710: 0.90625 1.13323\n",
      "Accuracy and loss at step 1720: 0.9375 0.996139\n",
      "Accuracy and loss at step 1730: 0.890625 1.1441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and loss at step 1740: 0.921875 1.06051\n",
      "Accuracy and loss at step 1750: 0.953125 0.882551\n",
      "Accuracy and loss at step 1760: 0.984375 0.906187\n",
      "Accuracy and loss at step 1770: 0.890625 1.16588\n",
      "Accuracy and loss at step 1780: 0.921875 1.01362\n",
      "Accuracy and loss at step 1790: 0.953125 1.05884\n",
      "Accuracy and loss at step 1800: 0.9375 1.07889\n",
      "Accuracy and loss at step 1810: 0.96875 0.867563\n",
      "Accuracy and loss at step 1820: 0.96875 0.918563\n",
      "Accuracy and loss at step 1830: 0.921875 0.971684\n",
      "Accuracy and loss at step 1840: 0.96875 0.888339\n",
      "Accuracy and loss at step 1850: 0.921875 1.07853\n",
      "Accuracy and loss at step 1860: 0.921875 1.08424\n",
      "Accuracy and loss at step 1870: 0.953125 0.878093\n",
      "Accuracy and loss at step 1880: 0.953125 0.920796\n",
      "Accuracy and loss at step 1890: 0.953125 0.97094\n",
      "Accuracy and loss at step 1900: 0.96875 0.972466\n",
      "Accuracy and loss at step 1910: 0.953125 0.901757\n",
      "Accuracy and loss at step 1920: 0.953125 1.00808\n",
      "Accuracy and loss at step 1930: 0.953125 0.888874\n",
      "Accuracy and loss at step 1940: 0.96875 0.948031\n",
      "Accuracy and loss at step 1950: 0.953125 0.952702\n",
      "Accuracy and loss at step 1960: 0.984375 0.854066\n",
      "Accuracy and loss at step 1970: 0.953125 0.89727\n",
      "Accuracy and loss at step 1980: 0.984375 0.986555\n",
      "Accuracy and loss at step 1990: 0.9375 0.97064\n",
      "Accuracy and loss at step 2000: 0.9375 1.05721\n",
      "Accuracy and loss at step 2010: 0.921875 1.13343\n",
      "Accuracy and loss at step 2020: 0.921875 0.982732\n",
      "Accuracy and loss at step 2030: 0.953125 0.896095\n",
      "Accuracy and loss at step 2040: 0.96875 0.956433\n",
      "Accuracy and loss at step 2050: 0.953125 0.942183\n",
      "Accuracy and loss at step 2060: 0.984375 0.92583\n",
      "Accuracy and loss at step 2070: 0.953125 0.941514\n",
      "Accuracy and loss at step 2080: 0.96875 0.951298\n",
      "Accuracy and loss at step 2090: 0.984375 0.883105\n",
      "Accuracy and loss at step 2100: 0.921875 0.965199\n",
      "Accuracy and loss at step 2110: 0.96875 1.00954\n",
      "Accuracy and loss at step 2120: 0.9375 0.879204\n",
      "Accuracy and loss at step 2130: 0.953125 0.999645\n",
      "Accuracy and loss at step 2140: 0.984375 0.909059\n",
      "Accuracy and loss at step 2150: 0.921875 1.09752\n",
      "Accuracy and loss at step 2160: 0.96875 1.03444\n",
      "Accuracy and loss at step 2170: 0.9375 0.972049\n",
      "Accuracy and loss at step 2180: 0.921875 1.08821\n",
      "Accuracy and loss at step 2190: 0.96875 0.929184\n",
      "Accuracy and loss at step 2200: 0.921875 1.19081\n",
      "Accuracy and loss at step 2210: 0.890625 1.16217\n",
      "Accuracy and loss at step 2220: 0.890625 1.13688\n",
      "Accuracy and loss at step 2230: 0.921875 1.03963\n",
      "Accuracy and loss at step 2240: 0.96875 0.952791\n",
      "Accuracy and loss at step 2250: 0.921875 0.982123\n",
      "Accuracy and loss at step 2260: 0.96875 1.04359\n",
      "Accuracy and loss at step 2270: 0.9375 1.01506\n",
      "Accuracy and loss at step 2280: 0.921875 0.962484\n",
      "Accuracy and loss at step 2290: 0.984375 0.827991\n",
      "Accuracy and loss at step 2300: 0.96875 0.781288\n",
      "Accuracy and loss at step 2310: 0.921875 1.08659\n",
      "Accuracy and loss at step 2320: 0.921875 1.02556\n",
      "Accuracy and loss at step 2330: 0.953125 0.945684\n",
      "Accuracy and loss at step 2340: 0.96875 1.04296\n",
      "Accuracy and loss at step 2350: 0.921875 1.1619\n",
      "Accuracy and loss at step 2360: 0.96875 0.976185\n",
      "Accuracy and loss at step 2370: 0.953125 1.01731\n",
      "Accuracy and loss at step 2380: 0.96875 0.939175\n",
      "Accuracy and loss at step 2390: 0.84375 1.44679\n",
      "Accuracy and loss at step 2400: 0.953125 0.881174\n",
      "Accuracy and loss at step 2410: 0.9375 1.02189\n",
      "Accuracy and loss at step 2420: 0.96875 0.926634\n",
      "Accuracy and loss at step 2430: 0.875 1.14357\n",
      "Accuracy and loss at step 2440: 0.96875 0.881661\n",
      "Accuracy and loss at step 2450: 0.9375 1.08858\n",
      "Accuracy and loss at step 2460: 0.953125 0.836524\n",
      "Accuracy and loss at step 2470: 0.890625 1.27917\n",
      "Accuracy and loss at step 2480: 0.921875 1.04407\n",
      "Accuracy and loss at step 2490: 0.984375 0.878748\n",
      "Accuracy and loss at step 2500: 0.96875 0.862582\n",
      "Accuracy and loss at step 2510: 0.984375 0.904289\n",
      "Accuracy and loss at step 2520: 0.9375 1.03973\n",
      "Accuracy and loss at step 2530: 0.90625 1.02304\n",
      "Accuracy and loss at step 2540: 0.96875 0.951429\n",
      "Accuracy and loss at step 2550: 0.921875 0.963692\n",
      "Accuracy and loss at step 2560: 0.9375 0.966718\n",
      "Accuracy and loss at step 2570: 0.953125 0.962099\n",
      "Accuracy and loss at step 2580: 0.953125 0.887802\n",
      "Accuracy and loss at step 2590: 0.90625 1.12454\n",
      "Accuracy and loss at step 2600: 0.984375 0.940061\n",
      "Accuracy and loss at step 2610: 0.953125 0.956598\n",
      "Accuracy and loss at step 2620: 0.953125 0.948454\n",
      "Accuracy and loss at step 2630: 0.953125 1.06342\n",
      "Accuracy and loss at step 2640: 0.921875 1.0627\n",
      "Accuracy and loss at step 2650: 0.984375 0.901656\n",
      "Accuracy and loss at step 2660: 0.984375 0.91606\n",
      "Accuracy and loss at step 2670: 0.9375 1.03493\n",
      "Accuracy and loss at step 2680: 0.921875 0.969209\n",
      "Accuracy and loss at step 2690: 0.90625 1.18058\n",
      "Accuracy and loss at step 2700: 0.96875 1.02812\n",
      "Accuracy and loss at step 2710: 0.921875 1.00699\n",
      "Accuracy and loss at step 2720: 0.9375 1.04102\n",
      "Accuracy and loss at step 2730: 0.953125 0.971891\n",
      "Accuracy and loss at step 2740: 0.90625 1.04914\n",
      "Accuracy and loss at step 2750: 0.953125 0.958087\n",
      "Accuracy and loss at step 2760: 0.921875 0.985093\n",
      "Accuracy and loss at step 2770: 0.953125 0.937367\n",
      "Accuracy and loss at step 2780: 0.921875 1.0207\n",
      "Accuracy and loss at step 2790: 0.9375 1.01303\n",
      "Accuracy and loss at step 2800: 0.953125 0.995826\n",
      "Accuracy and loss at step 2810: 0.953125 0.915627\n",
      "Accuracy and loss at step 2820: 0.890625 1.13131\n",
      "Accuracy and loss at step 2830: 1.0 0.780148\n",
      "Accuracy and loss at step 2840: 0.953125 0.871483\n",
      "Accuracy and loss at step 2850: 0.921875 1.01647\n",
      "Accuracy and loss at step 2860: 0.953125 0.927119\n",
      "Accuracy and loss at step 2870: 0.921875 1.04833\n",
      "Accuracy and loss at step 2880: 0.96875 0.918964\n",
      "Accuracy and loss at step 2890: 0.9375 0.979752\n",
      "Accuracy and loss at step 2900: 0.984375 0.833129\n",
      "Accuracy and loss at step 2910: 0.828125 1.34601\n",
      "Accuracy and loss at step 2920: 0.9375 1.01036\n",
      "Accuracy and loss at step 2930: 0.953125 0.963377\n",
      "Accuracy and loss at step 2940: 0.96875 0.986856\n",
      "Accuracy and loss at step 2950: 0.96875 0.876464\n",
      "Accuracy and loss at step 2960: 0.96875 0.885234\n",
      "Accuracy and loss at step 2970: 0.96875 0.86107\n",
      "Accuracy and loss at step 2980: 0.953125 0.920185\n",
      "Accuracy and loss at step 2990: 0.984375 0.825962\n",
      "Accuracy and loss at step 3000: 0.921875 1.06902\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "lr = 0.01\n",
    "while (i <= (3000)):\n",
    "    if i % 10 == 0:\n",
    "        summary, tr = sess.run([merged, optimizer], feed_dict=feed_dict(True, batch_size_tr, lr))\n",
    "        train_writer.add_summary(summary, i)\n",
    "        summary, acc, bat_loss = sess.run([merged, accuracy, loss], feed_dict=feed_dict(False, batch_size_ts))\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('Accuracy and loss at step %s: %s %s' % (i, acc, bat_loss))\n",
    "    else:\n",
    "        tr = sess.run(optimizer, feed_dict=feed_dict(True, batch_size_tr, lr))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3rc1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
